{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 토크나이징 with konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[출처 : 오늘코드](https://github.com/corazzon/KaggleStruggle/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jdk JVMNotFoundException (https://github.com/konlpy/konlpy/issues/24 \n",
    "# jdk 설치 https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "# .net framework 설치 : OS 맞게 설치필요 (https://www.microsoft.com/en-us/download/confirmation.aspx?id=40773)\n",
    "# MS build toold 설치 : https://visualstudio.microsoft.com/ko/downloads/?q=build\n",
    "# !pip install jpype1  # \n",
    "# !pip install konlpy, customized_konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어처리\n",
    "### [자연 언어 처리 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0_%EC%96%B8%EC%96%B4_%EC%B2%98%EB%A6%AC)\n",
    "* 인간이 발화하는 언어를 기계적으로 분석해서 자연어와 같이 구조화되지 않은 비정형 텍스트로부터 새로운 지식을 발견하는 과정\n",
    "\n",
    "NLP는?\n",
    "NLP(자연어처리)는 텍스트 문제에 접근하기 위한 기술집합이다.\n",
    "\n",
    "자연어처리(natural language processing, NLP) 분야는 인공지능의 큰 줄기 중에 하나입니다. 특히, 컴퓨터에게 사람이 사용하는 언어를 처리하고 이해하도록 함으로써, 사람과 컴퓨터 사이의 매개체 또는 인터페이스 역할을 할 수 있습니다. 따라서 컴퓨터 공학 뿐만 아니라, 언어학과 같은 다른 학문과의 융합적인 요소도 갖고 있습니다.\n",
    "\n",
    "따라서, 이러한 자연어처리의 세부적인 부분들이 합쳐져 최종적인 목표는 사람의 언어를 이해하여 컴퓨터로 하여금 여러가지 문제를 수행할 수 있도록 하는 것입니다. 컴퓨터는 이제 우리와 뗄 수 없는 존재가 되었고, 그러므로 이미 실제로 자연어처리는 우리의 일상에 가장 깊숙히 들어와 있는 분야이기도 합니다. 자연어처리 기술에 의해서 수행되는 대표적인 문제 또는 응용분야들은 다음과 같습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 자연어처리(NLP)와 관련 된 캐글 경진대회\n",
    "* [Sentiment Analysis on Movie Reviews | Kaggle](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)\n",
    "* [Toxic Comment Classification Challenge | Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "* [Spooky Author Identification | Kaggle](https://www.kaggle.com/c/spooky-author-identification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정제 Data Cleaning and Text Preprocessing\n",
    "기계가 텍스트를 이해할 수 있도록 텍스트를 정제해 준다.\n",
    "\n",
    "신호와 소음을 구분한다. 아웃라이어데이터로 인한 오버피팅을 방지한다.\n",
    "\n",
    "1. BeautifulSoup(뷰티풀숩)을 통해 HTML 태그를 제거\n",
    "2. 정규표현식으로 알파벳 이외의 문자를 공백으로 치환\n",
    "3. NLTK 데이터를 사용해 불용어(Stopword)를 제거\n",
    "4. 어간추출(스테밍 Stemming)과 음소표기법(Lemmatizing)의 개념을 이해하고 SnowballStemmer를 통해 어간을 추출\n",
    "\n",
    "\n",
    "### 텍스트 데이터 전처리 이해하기\n",
    "\n",
    "(출처 : [트위터 한국어 형태소 분석기](https://github.com/twitter/twitter-korean-text))\n",
    "\n",
    "**정규화 normalization (입니닼ㅋㅋ -> 입니다 ㅋㅋ, 샤릉해 -> 사랑해)**\n",
    "\n",
    "* 한국어를 처리하는 예시입니닼ㅋㅋㅋㅋㅋ -> 한국어를 처리하는 예시입니다 ㅋㅋ\n",
    "\n",
    "**토큰화 tokenization**\n",
    "\n",
    "* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어Noun, 를Josa, 처리Noun, 하는Verb, 예시Noun, 입Adjective, 니다Eomi ㅋㅋKoreanParticle\n",
    "\n",
    "**어근화 stemming (입니다 -> 이다)**\n",
    "\n",
    "* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어Noun, 를Josa, 처리Noun, 하다Verb, 예시Noun, 이다Adjective, ㅋㅋKoreanParticle\n",
    "\n",
    "\n",
    "**어구 추출 phrase extraction** \n",
    "\n",
    "* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어, 처리, 예시, 처리하는 예시\n",
    "\n",
    "Introductory Presentation: [Google Slides](https://docs.google.com/presentation/d/10CZj8ry03oCk_Jqw879HFELzOLjJZ0EOi4KJbtRSIeU/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma,Okt,Hannanum\n",
    "from konlpy.utils import pprint\n",
    "from pprint import pprint\n",
    "import ckonlpy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"존경하는 국민여러분. 성원해 주신 덕분에 평양에 잘 다녀왔습니다. \\n\\n국민 여러분께서 보셨듯이 정상회담에서 좋은 합의를 이뤘고, 최상의 환대를 받았습니다. 무엇보다 3일동안 김정은 위원장과 여러차례 만나 긴 시간 많은 대화를 허심탄회하게 나눌 수 있었던 것에 큰 의미를 두고 싶습니다.\\n\\n남북관계를 크게 진전시키고 두 정상 간의 신뢰 구축에도 큰 도움이 된 방문이었다고 평가하고 싶습니다.\\n\\n북측에서는 짧은 준비기간에도 불구하고 우리 대표단을 정성을 다해 맞아 주었습니다.\\n\\n오고 가는 동안 공항과 길가에서 열렬하게 환영해주고 환송해준 평양 시민들께 각별한 인사를 드리지 않을 수 없습니다.\\n\\n백두산에 오가는 동안 삼지연공항에서 따뜻하게 맞아주고 배웅해 준 지역 주민들께도 감사드립니다.\\n\\n저는 5월1일 경기장에서 열린 대규모 집단체조와 공연에서 15만 평양 시민들에게 대한민국 대통령으로써 사상 최초로 연설을 하는 기회를 가졌습니다.\\n\\n그들은 한반도를 영구히 핵무기와 핵위협이 없는 평화의 터전으로 만들어야 한다는 저의 연설에 대해 열렬한 박수를 보내줬습니다.\\n\\n존경하는 국민 여러분.\\n\\n지난 3일간 저는 김정은 위원장과 비핵화와 북미 대화에 대해서도 많은 대화를 나누었습니다. 첫날 회담에서도 대부분의 시간을 비핵화를 논의하는데 사용했습니다.\\n\\n김정은 위원장은 확고한 비핵화 의지를 거듭, 거듭 확약했습니다.\\n\\n가능한 한 빠른 시기에 완전한 비핵화를 끝내고 경제발전에 집중하고 싶다는 희망을 밝혔습니다.\\n\\n다만 북미정상회담에서 합의한 4개 합의사항이 함께 이행돼야 하므로 미국이 그 정신에 따라 상응하는 조치를 취해준다면, 영변 핵시설의 영구적 폐기를 포함한 추가적인 비핵화 조치를 계속 실행해나갈 용의가 있음을 표명했습니다.\\n\\n그리고 그 의지를 다시 한 번 분명하게 밝히는 차원에서 우선 동창리 미사일 엔진 시험장과 미사일 발사대를 유관국 전문가들의 참관 하에 영구적으로 폐기할 것을 확약했습니다.\\n\\n북한이 평양공동선언에서 사용한 '참관'이나 '영구적 폐기'라는 용어는, 결국 검증\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 북미정상회담 데이터 \n",
    "with open('./data/pyongyang_fin.txt', 'r', encoding='utf-8') as f:\n",
    "    texts = f.read()\n",
    "texts[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel , csv\n",
    "df = pd.read_excel('./data/xx.xlsx')\n",
    "# df = pd.read_csv('./data/xx.xlsx')\n",
    "\n",
    "# hive, db\n",
    "# conn = pyodbc.connect('DSN_B02_HIVE_ODBC;UID=da2_master;PWD=xxxx;charaset=utf8',autocommit=True)\n",
    "# sql = \"select * from xxx\"\n",
    "# df = pd.read_sql(sql, conn)\n",
    "\n",
    "# pyspark\n",
    "# df = hc.sql('select * from xxx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df['text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Kkma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2568d314f9e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m taggers = [('kkma',Kkma()),\n\u001b[0m\u001b[0;32m      2\u001b[0m            \u001b[1;33m(\u001b[0m\u001b[1;34m'twitter'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOkt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m            \u001b[1;33m(\u001b[0m\u001b[1;34m'hannanum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m            \u001b[1;31m# mecab은 window에서 지원안됨\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Kkma' is not defined"
     ]
    }
   ],
   "source": [
    "taggers = [('kkma',Kkma()),\n",
    "           ('twitter', Okt()),\n",
    "           ('hannanum', Hannanum())    \n",
    "           # mecab은 window에서 지원안됨\n",
    "]\n",
    "taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(taggers[0][1].pos('테스트 문장'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(taggers[1][1].pos('테스트 문장'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(taggers[2][1].pos('테스트 문장'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(taggers[2][1].tagset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pos tagging & word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = df_text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "token = []\n",
    "\n",
    "for name, tagger in taggers:\n",
    "    \n",
    "    process_time = time.time()\n",
    "    tokens.append([pos for sent in sents for pos in tagger.pos(sent)])\n",
    "    process_time = time.time() - process_time\n",
    "    \n",
    "    print('tagger name = {}, {:.3} secs'.format(name, process_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(token[0])\n",
    "count = {word:freq for word, freq in counter.items() if (freq>=4) and (word[1:2] == 'NN')}\n",
    "pprint(sorted(counter.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _), _tokens in zip(taggers, tokens):\n",
    "    print('pos {}'.format(name))\n",
    "    counter = Counter(_tokens)\n",
    "    counter = {word:freq for word, freq in counter.items() if (freq>=4) and (word[1:1] == 'N')}\n",
    "    pprint(sorted(counter.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Honeycrisp.jpg/1280px-Honeycrisp.jpg' width=200>\n",
    "* 이미지 출처 https://en.wikipedia.org/wiki/Apple\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
