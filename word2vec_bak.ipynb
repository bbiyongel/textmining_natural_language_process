{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word2Vec(Word Embedding to Vector)\n",
    "\n",
    "컴퓨터는 숫자만 인식할 수 있고 한글, 이미지는 바이너리 코드로 저장 됩니다.\n",
    "\n",
    "* one hot encoding(예 [0000001000]) 혹은 Bag of Word에서 vector size가 매우 크고 sparse 하므로 neural net 성능이 잘 나오지 않습니다.\n",
    "* `주위 단어가 비슷하면 해당 단어의 의미는 유사하다` 라는 아이디어\n",
    "* 단어를 트레이닝 시킬 때 주위 단어를 label로 매치하여 최적화\n",
    "* 단어를 `의미를 내포한 dense vector`로 매칭 시키는 것\n",
    "\n",
    "* Word2Vec은 분산 된 텍스트 표현을 사용하여 개념 간 유사성을 봅니다. \n",
    "* 예를 들어, 파리와 프랑스가 베를린과 독일이 (수도와 나라) 같은 방식으로 관련되어 있음을 이해합니다.\n",
    "\n",
    "![word2vec](https://1.bp.blogspot.com/-Q7F8ulD6fC0/UgvnVCSGmXI/AAAAAAAAAbg/MCWLTYBufhs/s1600/image00.gif)\n",
    "이미지 출처 : https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html\n",
    "\n",
    "* 단어의 임베딩과정을 실시간으로 시각화 : [word embedding visual inspector](https://ronxin.github.io/wevi/)\n",
    "\n",
    "\n",
    "![CBOW와 Skip-Gram](https://i.imgur.com/yXY1LxV.png)\n",
    "출처 : https://arxiv.org/pdf/1301.3781.pdf\n",
    " Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "\n",
    "## CBOW와 Skip-Gram\n",
    "* CBOW와 Skip-Gram기법이 있다.\n",
    "\n",
    "    * CBOW(continuous bag-of-words)는 전체 텍스트로 하나의 단어를 예측하기 때문에 작은 데이터셋일 수록 유리하다.    \n",
    "    * 아래 예제에서 __ 에 들어갈 단어를 예측한다.\n",
    "<pre>\n",
    "1) __가 맛있다. \n",
    "2) __를 타는 것이 재미있다. \n",
    "3) 평소보다 두 __로 많이 먹어서 __가 아프다.\n",
    "</pre>\n",
    "\n",
    "    * Skip-Gram은 타겟 단어들로부터 원본 단어를 역으로 예측하는 것이다. CBOW와는 반대로 컨텍스트-타겟 쌍을 새로운 발견으로 처리하고 큰 규모의 데이터셋을 가질 때 유리하다.\n",
    "    * `배`라는 단어 주변에 올 수 있는 단어를 예측한다.\n",
    "    \n",
    "    <pre>\n",
    "    1) *배*가 맛있다. \n",
    "    2) *배*를 타는 것이 재미있다. \n",
    "    3) 평소보다 두 *배*로 많이 먹어서 *배*가 아프다.\n",
    "    </pre>\n",
    "\n",
    "\n",
    "\n",
    "## Word2Vec 참고자료\n",
    "* [word2vec 모델 · 텐서플로우 문서 한글 번역본](https://tensorflowkorea.gitbooks.io/tensorflow-kr/g3doc/tutorials/word2vec/)\n",
    "* [Word2Vec으로 문장 분류하기 · ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)\n",
    "* [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/syllabus.html)\n",
    "* [Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "### 논문\n",
    "* [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "## Gensim\n",
    "\n",
    "* [gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* [gensim: Tutorials](https://radimrehurek.com/gensim/tutorial.html)\n",
    "* [한국어와 NLTK, Gensim의 만남 - PyCon Korea 2015](https://www.lucypark.kr/docs/2015-pyconkr/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim의 Word2Vec을 학습하기 위해서는 list of str 형식의 input이 필요합니다. 하지만 모든 리뷰들을 메모리에 올리지 않고도 학습할 수 있습니다. generator인 with open을 이용하여 \\__iter\\__를 오버라이딩한 CommentWord2Vec이라는 클래스를 생성합니다. \n",
    "\n",
    "\\__iter\\__에서 \n",
    "\n",
    "    for doc in f:\n",
    "        movie_idx, text, score = doc.split('\\t')\n",
    "        yield text.split()\n",
    "\n",
    "을 수행하면, 파일 f로부터 한 줄을 읽어 tap으로 텍스트를 구분한 뒤, 그 텍스트를 띄어쓰기 기준으로 잘라낸 token list가 for loop 안에서 출력됩니다. \n",
    "\n",
    "\\__iter\\__는 아래 구문이 작동할 수 있도록 해주는 파이썬 내장함수 입니다. \n",
    "\n",
    "    for doc in word2vec_corpus:\n",
    "        print(doc)\n",
    "\n",
    "토크나이징이 된 리뷰들을 CommentWord2Vec의 fname으로 입력하였기 때문에, 띄어쓰기 만으로 단어열을 yield 할 수 있습니다. 이러한 작업은 반복이 되기 때문에 raw corpus와 tokenized corpus를 구분하여 저장하는 것이 좋습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soynlp\n",
    "\n",
    "corpus_fname = '../../../data/comments_172movies//merged_comments.txt'\n",
    "tokenized_corpus_fname = '../../../data/comments_172movies/merged_comments_tokenized.txt'\n",
    "\n",
    "\n",
    "TRAIN_WORD2VEC = False\n",
    "word2vec_fname = '../../../data/comments_172movies/movie_review_word2vec_model_v2.2.pkl'\n",
    "\n",
    "\n",
    "TRAIN_DOC2VEC = False\n",
    "doc2vec_fname = '../../../data/comments_172movies/movie_review_doc2vec_model_v2.2.pkl'\n",
    "\n",
    "id2movie_fname = '../../../data/comments_172movies/id2movie.pkl'\n",
    "id2actor_fname = '../../../data/comments_172movies/id2actor.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus 파일은 tap으로 구분하여 한 줄에 <영화아이디, 텍스트, 평점>으로 입력되어 있습니다. \n",
    "\n",
    "    with open() as f:\n",
    "        docs=[doc.strip().split('\\t') for doc in f]\n",
    "\n",
    "위 구문은 파일을 읽어 한 줄에 줄바꿈 기호를 제거한 뒤, tap으로 줄을 분리합니다. 그 결과 한 줄에는 [영화아이디, 텍스트, 평점]의 리스트가 들어있게 됩니다. \n",
    "\n",
    "    [[영화아이디, 텍스트, 평점], \n",
    "     [영화아이디, 텍스트, 평점], \n",
    "     ...\n",
    "    ]\n",
    "\n",
    "위 형식의 list of list이기 때문에 각각의 column 별로 값을 분리하기 위하여 zip(\\*)을 이용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(fname, debug=True):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = []\n",
    "        for i, doc in enumerate(f):\n",
    "            if debug and i >= 1000:\n",
    "                break\n",
    "            docs.append(doc.split('\\t'))\n",
    "    \n",
    "    idx, texts, scores = zip(*docs)\n",
    "    return idx, texts, scores\n",
    "\n",
    "idx, docs, scores = get_text(tokenized_corpus_fname)\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "    \n",
    "class CommentWord2Vec:\n",
    "    \n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        if not os.path.exists(fname):\n",
    "            print('File not found: %s' % fname)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                movie_idx, text, score = doc.split('\\t')\n",
    "                yield text.split()\n",
    "                \n",
    "                \n",
    "word2vec_corpus = CommentWord2Vec(tokenized_corpus_fname)\n",
    "\n",
    "for num_doc, doc in enumerate(word2vec_corpus):\n",
    "    if num_doc > 5: break\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "if TRAIN_WORD2VEC:\n",
    "    word2vec_model = Word2Vec(word2vec_corpus,\n",
    "                              size=100,\n",
    "                              alpha=0.025,\n",
    "                              window=5,\n",
    "                              min_count=5,\n",
    "                              sg=0,\n",
    "                              work\n",
    "                              negative=5)\n",
    "    with open(word2vec_fname, 'wb') as f:\n",
    "        pickle.dump(word2vec_model, f)\n",
    "        \n",
    "else:\n",
    "    with open(word2vec_fname, 'rb') as f:\n",
    "        word2vec_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim의 Word2Vec을 이용합니다. 미리 만들어둔 word2vec_corpus를 Word2Vec의 argument로 입력합니다. default parameters를 이용하여 Word2Vec을 학습힙니다. \n",
    "\n",
    "Word2Vec의 arguments 중에서 중요한 것들은 아래와 같습니다. \n",
    "\n",
    "- size: 단어의 임베딩 공간의 크기\n",
    "- alpha: learning rate\n",
    "- window: 한 단어의 좌/우의 문맥 크기\n",
    "- min_count: 모델이 학습할 단어의 최소 출현 빈도수\n",
    "- max_vocab_size: None이 아닌 숫자를 입력하면 빈도수 기준으로 상위 max_vocab_size 개수만큼의 단어만 학습\n",
    "- sg: 1이면 skipgram 이용\n",
    "- negative: negative sampling에서 negative sample의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv['영화']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['영화']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec에서 index2word의 단어 순서는 단어의 빈도순입니다. 단어에 관련된 정보를 가져오고 싶다면 Word2Vec.wv.vocab을 이용하세요. \n",
    "\n",
    "vocab은 {단어: Vocab} 형식의 dict입니다. Vocab은 namedtuple이기 때문에 아래와 같이 이용할 수 있습니다. \n",
    "\n",
    "    word2vec_model.wv.vocab['영화'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['영화']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(word2vec_model.wv.vocab['영화'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['영화'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장많이 등장한 word\n",
    "word2vec_model.wv.index2word[1], word2vec_model.wv.index2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(word2vec_model.wv.vocab['이']))\n",
    "print(str(word2vec_model.wv.vocab['관람객']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar('하정우', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 Word2Vec모델의 .most_similar(단어, topn) 함수는 입력된 단어에 대하여 가장 비슷한 topn개의 다른 단어들과 유사도를 출력합니다. \n",
    "\n",
    "아래의 에제에서 '하정우'와 가장 비슷한 단어는 '이정재'이며, 유사도는 0.7440입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1점과 유사한 단어가 한글로 쓴 '일점', 그 이후로는 1, 2, 3, ... 이렇게 점수가 멀어져가는 것도 볼 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'십점' 이라는 말은 점수가 들어갈 수 있는 문맥에서 나오는 말이기도 하지만, 긍정적인 표현에서 더 많이 나왔을 것입니다. 그렇기 때문에 1점 보다도, 천점, 백점 같은 단어들이 더 유사하게 학습됩니다 (네이버 영화에서 10점은 별 다섯개입니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메가박스의 유사어로 롯데시네마가 잡히기도 하지만, **롯시**라는 약어도 학습됩니다. 토크나이징이 잘 되어 있다면, 이와 같이 예상하지 못하는 단어도 비슷한 단어로 학습될 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당연히 Word2Vec 입장에서는 언어가 섞여 씌여도 관계가 없습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec을 학습하기 위해서는 각각 문서의 label이 저장되어야 합니다. 이를 위하여 TaggedDocument라는 클래스가 이용됩니다. TaggedDocument는 단어들을 words에, 레이블 정보를 tags에 리스트 형태로 입력합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec은 Word2Vec과 동일하게 단어 벡터를 학습하며, 이를 바탕으로 document vector를 학습합니다. 그렇기 때문에 단어 벡터에 의한 유사도는 동일하게 학습됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "class CommentDoc2Vec:\n",
    "    \n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        if not os.path.exists(fname):\n",
    "            print('File not found: %s' % fname)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                movie_idx, text, score = doc.split('\\t')\n",
    "                yield TaggedDocument(words=text.split(),\n",
    "                                     tags=['MOVIE_%s' % movie_idx])\n",
    "\n",
    "                \n",
    "doc2vec_corpus = CommentDoc2Vec(tokenized_corpus_fname)\n",
    "\n",
    "\n",
    "if TRAIN_DOC2VEC:\n",
    "    doc2vec_model = Doc2Vec(doc2vec_corpus)\n",
    "    with open(doc2vec_fname, 'wb') as f:\n",
    "        pickle.dump(doc2vec_model, f)\n",
    "\n",
    "else:\n",
    "    with open(doc2vec_fname, 'rb') as f:\n",
    "        doc2vec_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(doc2vec_corpus):\n",
    "    if i > 3: break\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Most similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.most_similar('영화', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc2vec_model.docvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec model의 .docvecs안에는 document vector와 관련된 정보들이 저장되어 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doctag in sorted(doc2vec_model.docvecs.doctags.items(), key=lambda x:x[1].offset):\n",
    "    print(idx, doctag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doctags에 들어있는 offset은 document vector의 임베딩 메트릭스의 row id이며, word_count는 각 태그에 해당하는 문서에 단어가 몇 개 있었는지, doc_count는 각 태그에 해당하는 문서가 몇 번 등장하였는지입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs.most_similar('MOVIE_59845')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 MOVIE_59845의 offset = 1은 docvec에서의 row id가 1라는 의미입니다. \n",
    "\n",
    "리스트 안의 tuple 값의 의미는 (doctag, 유사도) 입니다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document vector의 row id로도 most_similar를 찾을 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs.most_similar(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc2vec_model.docvecs.doctags)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dov2Veco.docvecs.doctags는 document vector의 각 row 기준으로 tag 정보가 저장되어 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc2vec_model.docvecs.doctags.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_count는 해당 문서에 등장한 단어의 총 빈도수의 합이며, doc_count는 MOVIE\\_%D 형식으로 입력된 TaggedDocument의 개수입니다. 우리 데이터에서는 영화별 리뷰 개수에 해당합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec 해석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(id2movie_fname, 'rb') as f:\n",
    "    idx2movie = pickle.load(f)\n",
    "    \n",
    "movie2idx = {name:movie_idx for movie_idx, name in idx2movie.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for similar in doc2vec_model.docvecs.most_similar('MOVIE_134963'):\n",
    "    print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec.docvecs.most_similar에서는 태그로 구분되는 문서들에 대하여 유사한 다른 문서를 찾아줍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 아이디를 영화 제목으로 바꿔서 해석해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doctag in doc2vec_model.docvecs.doctags:\n",
    "    movie_idx = doctag.split('_')[1]\n",
    "    name = idx2movie.get(movie_idx, -1)\n",
    "    print(movie_idx, '\\t', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2name(similar):\n",
    "    idx = similar[0].split('_')[1]\n",
    "    return (idx2movie.get(idx, 'unknown'), idx, similar[1])\n",
    "\n",
    "\n",
    "print('라라랜드\\n')\n",
    "\n",
    "for similar in doc2vec_model.docvecs.most_similar('MOVIE_134963'):\n",
    "    print(id2name(similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 리뷰를 기준으로 각 영화를 document vector로 표현하였을 때 라라랜드와 리뷰가 비슷한 영화는 '비긴 어게인', '어바웃 타임' 등입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('광해 왕이된 남자\\n')\n",
    "for similar in doc2vec_model.docvecs.most_similar('MOVIE_83893'):\n",
    "    print(id2name(similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('아바타\\n')\n",
    "for similar in doc2vec_model.docvecs.most_similar('MOVIE_62266'):\n",
    "    print(id2name(similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE를 이용한 리뷰가 비슷한 영화의 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs.doctag_syn0.shape, type(doc2vec_model.docvecs.doctag_syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = [doc2vec_model.docvecs.index_to_doctag(i) for i in range(172)]\n",
    "print(index2tag[:5])\n",
    "\n",
    "tag2index = {tag:index for index, tag in enumerate(index2tag)}\n",
    "\n",
    "index2name = [idx2movie.get(tag.split('_')[1], -1) for tag in index2tag]\n",
    "print(index2name[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 문서 시각화 시간에 살펴보았던 TSNE를 이용하여 document vector를 2차원으로 임베딩을 하여 시각화를 수행합니다. 시간이 오래 걸릴 작업은 %%time을 적어 수행 시간을 출력하면 편합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "y_tsne = tsne.fit_transform(doc2vec_model.docvecs.doctag_syn0)\n",
    "\n",
    "y_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(list(matplotlib.font_manager.get_fontconfig_fonts())[:5])\n",
    "\n",
    "krfont = {'family' : 'nanumgothic', 'weight' : 'bold', 'size'   : 10}\n",
    "matplotlib.rc('font', **krfont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라라랜드나 뷰티 인사이드와 같이 멜로물의 영화들이 한쪽에 몰려 위치함을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('134963 = ', idx2movie['134963'])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_tsne[:,0], y_tsne[:,1], color='red')\n",
    "\n",
    "similars = [tag for tag, _ in doc2vec_model.docvecs.most_similar('MOVIE_134963')] + ['MOVIE_134963']\n",
    "\n",
    "for tag in similars:\n",
    "    index = tag2index.get(tag, -1)\n",
    "    if index == -1:\n",
    "        continue\n",
    "    name = index2name[index]\n",
    "    plt.annotate(name, (y_tsne[index, 0], y_tsne[index, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관상의 유사한 영화들도 한쪽에 몰려 위치하고 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('93728 = ', idx2movie['93728'])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_tsne[:,0], y_tsne[:,1], color='red')\n",
    "\n",
    "similars = [tag for tag, _ in doc2vec_model.docvecs.most_similar('MOVIE_93728')] + ['MOVIE_93728']\n",
    "\n",
    "for tag in similars:\n",
    "    index = tag2index.get(tag, -1)\n",
    "    if index == -1:\n",
    "        continue\n",
    "    name = index2name[index]\n",
    "    plt.annotate(name, (y_tsne[index, 0], y_tsne[index, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "액션적인 요소가 많았던 영화들은 앞선 라라랜드나 비긴어게인과 같은 영화들과는 떨어져서, 액션 영화들끼리 뭉쳐있음을 확인할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('62266 = ', idx2movie['62266'])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_tsne[:,0], y_tsne[:,1], color='red')\n",
    "\n",
    "similars = [tag for tag, _ in doc2vec_model.docvecs.most_similar('MOVIE_62266')] + ['MOVIE_62266']\n",
    "\n",
    "for tag in similars:\n",
    "    index = tag2index.get(tag, -1)\n",
    "    if index == -1:\n",
    "        continue\n",
    "    name = index2name[index]\n",
    "    plt.annotate(name, (y_tsne[index, 0], y_tsne[index, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
